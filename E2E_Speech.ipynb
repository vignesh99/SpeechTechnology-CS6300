{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "E2E Speech.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM59ucbjiZTSVXeIPiH48WQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vignesh99/SpeechTechnology-CS6300/blob/master/E2E_Speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzoeJS1PTkFg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "0435b389-abcc-42c3-d35d-61fda908a668"
      },
      "source": [
        "                                    #Get data files onto root\n",
        "%cd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# !cp -avr /content/gdrive/My\\ Drive/TIMIT\\ E2E/timit.zip /root\n",
        "# !unzip /root/timit.zip -d /root\n",
        "!cp -avr /content/gdrive/My\\ Drive/TIMIT\\ E2E/Wordset\\ files/* /root\n",
        "!cp -avr /content/gdrive/My\\ Drive/TIMIT\\ E2E/Wordceps\\ files/* /root\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "Mounted at /content/gdrive\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordset files/inds.pkl' -> '/root/inds.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordset files/reps.pkl' -> '/root/reps.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordset files/wordset.pkl' -> '/root/wordset.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordset files/words.pkl' -> '/root/words.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/lenframes.pkl' -> '/root/lenframes.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/testfreqs.pkl' -> '/root/testfreqs.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/timitceps.pkl' -> '/root/timitceps.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/trainfreqs.pkl' -> '/root/trainfreqs.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/wordloop.pkl' -> '/root/wordloop.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/words.pkl' -> '/root/words.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/Xtest.pkl' -> '/root/Xtest.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/Xtrain.pkl' -> '/root/Xtrain.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/Ytest.pkl' -> '/root/Ytest.pkl'\n",
            "'/content/gdrive/My Drive/TIMIT E2E/Wordceps files/Ytrain.pkl' -> '/root/Ytrain.pkl'\n",
            "inds.pkl       testfreqs.pkl   wordloop.pkl  Xtest.pkl\t Ytrain.pkl\n",
            "lenframes.pkl  timitceps.pkl   wordset.pkl   Xtrain.pkl\n",
            "reps.pkl       trainfreqs.pkl  words.pkl     Ytest.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kh6pa0hDOKab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "8785e37a-db93-4034-da1f-d2c7c93a71be"
      },
      "source": [
        "                                    #Import libraries\n",
        "from pylab import *\n",
        "import os\n",
        "from google.colab import files\n",
        "!pip install pickle-mixin           #Use pkl to store model\n",
        "from pickle import dump\n",
        "from pickle import load      \n",
        "!pip install SoundFile\n",
        "import soundfile as sf              #For .wav to data\n",
        "from scipy.io import wavfile\n",
        "!pip install python_speech_features\n",
        "from python_speech_features import mfcc\n",
        "import pandas as pd\n",
        "import csv\n",
        "import random\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pickle-mixin\n",
            "  Downloading https://files.pythonhosted.org/packages/02/77/9d5eb2201bbc130e2a5cc41fc949e4ab0da74b619107eac1c511be3af7a7/pickle-mixin-1.0.2.tar.gz\n",
            "Building wheels for collected packages: pickle-mixin\n",
            "  Building wheel for pickle-mixin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle-mixin: filename=pickle_mixin-1.0.2-cp36-none-any.whl size=6000 sha256=51414bff21851cb20b3ca5a90f4aa9b10f92bc0ee97a5df639de3b06d9c4a60d\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/05/42/71de70fa36b9cbb7657bb5793a16f8028c1cdc1bdd3b8e1ac3\n",
            "Successfully built pickle-mixin\n",
            "Installing collected packages: pickle-mixin\n",
            "Successfully installed pickle-mixin-1.0.2\n",
            "Collecting SoundFile\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from SoundFile) (1.14.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->SoundFile) (2.20)\n",
            "Installing collected packages: SoundFile\n",
            "Successfully installed SoundFile-0.10.3.post1\n",
            "Collecting python_speech_features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-speech-features: filename=python_speech_features-0.6-cp36-none-any.whl size=5887 sha256=aaa83ffbef8f9d465b95e1a0f65c8a10d43e4e649d21f51c1f3ae9cf3f281b9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD8VTPiLApfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Import torch requirements\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader,TensorDataset"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jELsKCAywUV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "58dbbe00-66a2-44e0-ab2c-85ccd5919ba2"
      },
      "source": [
        "#words = load(open('words.pkl', 'rb'))\n",
        "#inds = load(open('inds.pkl', 'rb'))\n",
        "reps = load(open('reps.pkl', 'rb'))\n",
        "wordset = load(open('wordset.pkl', 'rb'))\n",
        "\n",
        "Nreps = 250\t\t\t\t\t\t\t#Number of repititions of each word\n",
        "\n",
        "inds = np.where(reps >= Nreps)[0]\n",
        "words = wordset[inds]               #Choose only 36-most-repeated words\n",
        "freqs = reps[inds]                  #Frequency at which these words occur\n",
        "print(len(inds))\n",
        "print(words)\n",
        "print(freqs)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "['a' 'all' 'an' 'and' 'ask' 'carry' 'dark' \"don't\" 'greasy' 'had' 'in'\n",
            " 'is' 'like' 'me' 'of' 'oily' 'rag' 'she' 'suit' 'that' 'the' 'to' 'wash'\n",
            " 'water' 'year' 'you' 'your']\n",
            "[ 866  543  571  481  464  463  465  488  462  526  944  399  511  505\n",
            "  454  470  470  571  462  611 1602 1018  469  479  472  271  565]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k56Jx8fdWEnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mostrepeatedwords(Nrep=250) :\n",
        "    allwords = []\n",
        "                                    #Go through all folders and consolidate the words\n",
        "    for subdir, dirs, files in os.walk(\"timit_train\"):\n",
        "        dirname = subdir.split(os.path.sep)[-1]\n",
        "        print('Directory:', dirname)\n",
        "        for f in files:\n",
        "            if f.endswith('.txt'):\n",
        "                                    #Get words of each sentence from .txt file\n",
        "                !cp -avr /root/timit_train/$dirname/$f /root\n",
        "                words = np.genfromtxt(f,delimiter=' ',dtype=str)\n",
        "                words = words[2:]\n",
        "                words[-1] = words[-1][:-1]\n",
        "                                    #Put all the words in a list\n",
        "                allwords += list(map(str.lower,list(words)))\n",
        "                !rm $f\n",
        "                                    #Get the unique set of words and their counts\n",
        "    wordunique,wordcount = np.unique(allwords,return_counts=True)\n",
        "                                    #Choose only those words with significant repitions\n",
        "    indrep = np.where(wordcount >= Nrep)[0]\n",
        "    #print(len(indrep))\n",
        "    wordclass = wordunique[indrep]\n",
        "    return wordclass,wordunique,wordcount,indrep"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbwWxT6SbZk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# words,wordset,reps,inds = mostrepeatedwords()\n",
        "# words"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8YdW3uUftVj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Dump words model file\n",
        "# dump(words, open('words.pkl', 'wb'))\n",
        "# dump(inds, open('inds.pkl', 'wb'))\n",
        "# dump(reps, open('reps.pkl', 'wb'))\n",
        "# dump(wordset, open('wordset.pkl', 'wb'))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4JAhCRT2x6-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Download model files\n",
        "# files.download(\"/root/words.pkl\")\n",
        "# files.download(\"/root/inds.pkl\")\n",
        "# files.download(\"/root/reps.pkl\")\n",
        "# files.download(\"/root/wordset.pkl\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0pG-pv64GrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Getting the mfcc features for different words\n",
        "def wavtoMfcc(words,Nmfcc=38) :\n",
        "    count = 0                       #Initialize required lists\n",
        "    timit_audio = []\n",
        "    new_timit_audio = []\n",
        "    timitceps = []\n",
        "    wordloop = []\n",
        "    lenframes = []\n",
        "    for subdir, dirs, files in os.walk('timit_train'):\n",
        "        dirname = subdir.split(os.path.sep)[-1]\n",
        "        for f in files:\n",
        "            if f.endswith('.wav'):\n",
        "                count += 1\n",
        "                print(os.path.join(subdir, f), ', name:', f, ', Count:', count)\n",
        "                                    #Read data from the .wav file\n",
        "                data, fs = sf.read(os.path.join(subdir, f))\n",
        "                timit_audio.append(data)\n",
        "                fword = f[:-4]+\".wrd\"\n",
        "                                    #Check word files for most repeated words\n",
        "                print(os.path.join(subdir, fword), ', name:', fword, ', Count:', count)\n",
        "                word_file = np.array(pd.read_csv(os.path.join(subdir, fword), delimiter = ' ',header = None))\n",
        "                new_data = []\n",
        "                for w in np.array(word_file[:,2]):\n",
        "                    if w in words:\n",
        "                        a = np.where(word_file[:,2] == w)\n",
        "                        start = word_file[a[0],0][0]\n",
        "                        end = word_file[a[0],1][0]\n",
        "                        if(start != end):   #For removing corner case error\n",
        "                                    #Collect the .wav samples of required slices\n",
        "                            new_data = data[start:end]\n",
        "                            new_timit_audio.append(new_data)\n",
        "                                            #Convert them to mfcc\n",
        "                            ceps = mfcc(new_data, fs, numcep=Nmfcc,winlen=0.008,winstep=0.004,nfilt=76)\n",
        "                            timitceps.append(ceps)\n",
        "                                    #Put the words in order of the mfcc features\n",
        "                            wordloop.append(w)\n",
        "                            lenframes.append(len(ceps))\n",
        "    \n",
        "    timitceps = array(timitceps)\n",
        "    wordloop = array(wordloop)\n",
        "    lenframes = array(lenframes) \n",
        "    maxframes = lenframes.max()     #Obtain maximum size of frames\n",
        "\n",
        "    return timitceps,wordloop,lenframes,maxframes,new_timit_audio                                           "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0H_FaRryK-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# timitceps,wordloop,lenframes,maxframes,new_timit_audio = wavtoMfcc(words)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUwguHR7kEtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Dump processed files\n",
        "# dump(timitceps, open('timitceps.pkl', 'wb'))\n",
        "# dump(wordloop, open('wordloop.pkl', 'wb'))\n",
        "# dump(lenframes, open('lenframes.pkl', 'wb'))\n",
        "# dump(new_timit_audio, open('new_timit_audio.pkl', 'wb'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydmhgQtpkfHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Download model files\n",
        "# files.download(\"/root/timitceps.pkl\")\n",
        "# files.download(\"/root/wordloop.pkl\")\n",
        "# files.download(\"/root/lenframes.pkl\")\n",
        "# files.download(\"/root/new_timit_audio.pkl\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkFtniQitoh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Load the required files\n",
        "# timitceps = load(open('timitceps.pkl', 'rb'))\n",
        "# wordloop = load(open('wordloop.pkl', 'rb'))\n",
        "# lenframes = load(open('lenframes.pkl', 'rb'))\n",
        "# new_timit_audio = load(open('new_timit_audio.pkl', 'rb'))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auI0KrIyTPby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mfcctotensor(words,trainfreqs,testfreqs,timitceps,wordloop,lenframes,maxframes=162,Nmfcc=38) :\n",
        "                                    #Declare train and test arrays\n",
        "    Xtrain = np.zeros((sum(trainfreqs),maxframes,Nmfcc))\n",
        "    Xtest = np.zeros((sum(testfreqs),maxframes,Nmfcc))\n",
        "    Ytrain = np.zeros(sum(trainfreqs))\n",
        "    Ytest = np.zeros(sum(testfreqs))\n",
        "    indtrain = cumsum(trainfreqs)   #Cumulative sum of indices\n",
        "    indtest = cumsum(testfreqs)\n",
        "                                    #Loop to collect train and test features\n",
        "    for i in range(0,len(words)) :\n",
        "    \n",
        "                                    #Obtain the particular word's mfcc features\n",
        "        indcep = np.where(wordloop==words[i])[0]\n",
        "        indlen = lenframes[indcep]\n",
        "                                    #Can add randomness if needed (while choosing for train and test)\n",
        "        trainindcep = indcep[:trainfreqs[i]]\n",
        "        testindcep = indcep[trainfreqs[i]:]\n",
        "        trainindlen = indlen[:trainfreqs[i]]\n",
        "        testindlen = indlen[trainfreqs[i]:]\n",
        "                                    #Check if occurance match\n",
        "        if trainfreqs[i] + testfreqs[i] == len(indcep) :            \n",
        "                                    #Loop over train features and transfer from list to array\n",
        "            for j in range(0,len(trainindcep)) :\n",
        "                if i != 0 :\n",
        "                    Xtrain[indtrain[i-1]+j][:trainindlen[j]] = timitceps[trainindcep[j]]\n",
        "                else :\n",
        "                    Xtrain[j][:trainindlen[j]] = timitceps[trainindcep[j]]\n",
        "                                    #Loop over test features and transfer from list to array\n",
        "            for j in range(0,len(testindcep)) :\n",
        "                if i != 0 :\n",
        "                    Xtest[indtest[i-1]+j][:testindlen[j]] = timitceps[testindcep[j]]\n",
        "                else :\n",
        "                    Xtest[j][:testindlen[j]] = timitceps[testindcep[j]]\n",
        "        else :\n",
        "            print(\"ERROR : Frequencies don't match with index length\")\n",
        "            return None\n",
        "                                    #Assign outputs for ANN\n",
        "        if i == 0 :\n",
        "            Ytrain[:indtrain[i]] = i\n",
        "            Ytest[:indtest[i]] = i\n",
        "        else :\n",
        "            Ytrain[indtrain[i-1]:indtrain[i]] = i\n",
        "            Ytest[indtest[i-1]:indtest[i]] = i\n",
        "\n",
        "    return Xtrain,Ytrain,Xtest,Ytest,words,trainfreqs,testfreqs"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2iJu6sb6-29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def traintesttensors(timitceps,wordloop,lenframes,Nmfcc=38) :\n",
        "                                    #Get required inputs\n",
        "    words,freqs = np.unique(wordloop,return_counts=True)\n",
        "    trainfreqs = np.round(0.8*freqs.astype(\"float\"))\n",
        "    testfreqs = freqs-trainfreqs\n",
        "    maxframes = lenframes.max()\n",
        "\n",
        "    return mfcctotensor(words,trainfreqs.astype(\"int\"),testfreqs.astype(\"int\"),timitceps,wordloop,lenframes,maxframes)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlbS2-3pBfcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Xtrain,Ytrain,Xtest,Ytest,words,trainfreqs,testfreqs = traintesttensors(timitceps,wordloop,lenframes)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poaoGPDLEp29",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Dump processed files\n",
        "# dump(Xtrain, open('Xtrain.pkl', 'wb'))\n",
        "# dump(Ytrain, open('Ytrain.pkl', 'wb'))\n",
        "# dump(Xtest, open('Xtest.pkl', 'wb'))\n",
        "# dump(Ytest, open('Ytest.pkl', 'wb'))\n",
        "# dump(words, open('words.pkl', 'wb'))\n",
        "# dump(trainfreqs, open('trainfreqs.pkl', 'wb'))\n",
        "# dump(testfreqs, open('testfreqs.pkl', 'wb'))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZTzZ-d5FqFr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Download model files\n",
        "# files.download(\"/root/Xtrain.pkl\")\n",
        "# files.download(\"/root/Ytrain.pkl\")\n",
        "# files.download(\"/root/Xtest.pkl\")\n",
        "# files.download(\"/root/Ytest.pkl\")\n",
        "# files.download(\"/root/words.pkl\")\n",
        "# files.download(\"/root/trainfreqs.pkl\")\n",
        "# files.download(\"/root/testfreqs.pkl\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByWYHFh4KfEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                    #Load files\n",
        "Xtrain = load(open('Xtrain.pkl', 'rb'))\n",
        "Ytrain = load(open('Ytrain.pkl', 'rb'))\n",
        "Xtest = load(open('Xtest.pkl', 'rb'))\n",
        "Ytest = load(open('Ytest.pkl', 'rb'))\n",
        "words = load(open('words.pkl', 'rb'))\n",
        "# trainfreqs = load(open('trainfreqs.pkl', 'rb'))\n",
        "# testfreqs = load(open('testfreqs.pkl', 'rb'))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIL2dyjDDKxY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size,feature_size,batch_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feature_size = feature_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.gru = nn.GRU(feature_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output, hidden = self.gru(input, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.batch_size, self.hidden_size, device=device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXB-Be1La4VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, feature_size, output_size,batch_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.feature_size = feature_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.gru = nn.GRU(feature_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        #print(input.size())\n",
        "        output = F.relu(input)\n",
        "        #output = input\n",
        "        #print(output.size())\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        #print(output.size())\n",
        "        output = self.out(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.batch_size, self.hidden_size, device=device)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20FRjzA9WYj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 162\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,feature_size, max_length=MAX_LENGTH):\n",
        "                                    #Initialize hidden vector\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "                                    #Make the gradients zero\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "                                    #No. of windows/ dependent units\n",
        "    input_length = input_tensor.size(1)\n",
        "    target_length = target_tensor.size(1)\n",
        "                                    #Storing outputs for attention\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "                                    #Loop over the no. of windows\n",
        "    for ei in range(input_length):\n",
        "                                    #Call encoder (?)\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[:,ei].unsqueeze(0), encoder_hidden)\n",
        "            #input_tensor[:,ei].view(1,-1,feature_size), encoder_hidden)\n",
        "        #print(encoder_output[0,0])\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "                                    #Initialize decoder input (?)\n",
        "    decoder_input = torch.zeros(1,input_tensor.size(0),input_tensor.size(2), device=device)\n",
        "    \n",
        "                                    #Use common hidden layer\n",
        "    decoder_hidden = encoder_hidden\n",
        "    #print(encoder_hidden.size())\n",
        "\n",
        "    use_teacher_forcing = True#True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "                                    #Call decoder (?)\n",
        "            #print(decoder_input.size())\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden)\n",
        "                                    #Comparing same decoder output with different target vectors (?)\n",
        "            loss += criterion(decoder_output, target_tensor[:,di].unsqueeze(0))\n",
        "                                    #Teacher forcing\n",
        "            decoder_input = target_tensor[:,di].unsqueeze(0)\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input.unsqueeze(0), decoder_hidden)\n",
        "            #topv, topi = decoder_output.topk(1)\n",
        "            #decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[:,di].unsqueeze(0))\n",
        "            # if decoder_input.item() == EOS_token:\n",
        "            #     break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VliSeCiVv0dU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    #loc = matplotlib.ticker.MultipleLocator(base=0.2)\n",
        "    #ax.yaxis.set_major_locator(loc)\n",
        "    plt.semilogy(points)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEfEu5P0sNP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(dataloader, encoder, decoder, n_iters,feature_size, print_every=1, plot_every=1, learning_rate=0.001):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    batchloss = 0                   #Reset after every epoch\n",
        "    print_loss_total = 0            #Reset every print_every\n",
        "    plot_loss_total = 0             #Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        for i,input in enumerate(dataloader) :\n",
        "            input_tensor = input[0]\n",
        "            target_tensor = input[1]\n",
        "\n",
        "            loss = train(input_tensor, target_tensor, encoder,\n",
        "                        decoder, encoder_optimizer, decoder_optimizer, criterion,feature_size)\n",
        "            batchloss += loss\n",
        "        print_loss_total += batchloss\n",
        "        plot_loss_total += batchloss\n",
        "        batchloss = 0\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "                                    #Make it zero after every print\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                        iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    #print(plot_losses)\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKzx9YxMmM8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41ab47dc-2b38-433e-8738-31e16e3d89ef"
      },
      "source": [
        "                                    #Create dataloader\n",
        "trainIp = torch.from_numpy(Xtrain[:1000]).float().to(device)\n",
        "traindata = TensorDataset(trainIp,trainIp)\n",
        "dataloader = DataLoader(dataset=traindata,\n",
        "                        batch_size=20,\n",
        "                        shuffle=True,\n",
        "                        drop_last=True)\n",
        "hidden_size = 256\n",
        "feature_size = 38\n",
        "output_size = 38\n",
        "batch_size = 20\n",
        "device = torch.device('cuda')\n",
        "encoder = EncoderRNN(hidden_size,feature_size,batch_size).to(device)\n",
        "decoder = DecoderRNN(hidden_size,feature_size,output_size,batch_size).to(device)\n",
        "trainIters(dataloader,encoder, decoder, 120,feature_size)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0m 11s (- 22m 47s) (1 0%) 2427.8442\n",
            "0m 23s (- 22m 41s) (2 1%) 1957.5138\n",
            "0m 34s (- 22m 29s) (3 2%) 1752.0534\n",
            "0m 46s (- 22m 17s) (4 3%) 1631.4611\n",
            "0m 57s (- 22m 3s) (5 4%) 1559.1637\n",
            "1m 8s (- 21m 50s) (6 5%) 1504.7926\n",
            "1m 20s (- 21m 40s) (7 5%) 1461.2966\n",
            "1m 32s (- 21m 29s) (8 6%) 1436.0647\n",
            "1m 43s (- 21m 17s) (9 7%) 1410.2287\n",
            "1m 55s (- 21m 6s) (10 8%) 1395.2342\n",
            "2m 6s (- 20m 56s) (11 9%) 1385.3030\n",
            "2m 18s (- 20m 44s) (12 10%) 1367.1233\n",
            "2m 29s (- 20m 33s) (13 10%) 1356.0595\n",
            "2m 41s (- 20m 22s) (14 11%) 1346.2867\n",
            "2m 53s (- 20m 11s) (15 12%) 1338.3442\n",
            "3m 4s (- 19m 59s) (16 13%) 1335.5308\n",
            "3m 16s (- 19m 48s) (17 14%) 1324.6438\n",
            "3m 27s (- 19m 37s) (18 15%) 1317.8240\n",
            "3m 39s (- 19m 26s) (19 15%) 1315.3931\n",
            "3m 50s (- 19m 14s) (20 16%) 1310.2945\n",
            "4m 2s (- 19m 3s) (21 17%) 1302.5039\n",
            "4m 14s (- 18m 53s) (22 18%) 1296.0322\n",
            "4m 25s (- 18m 41s) (23 19%) 1292.4670\n",
            "4m 37s (- 18m 29s) (24 20%) 1289.8247\n",
            "4m 49s (- 18m 18s) (25 20%) 1283.2206\n",
            "5m 0s (- 18m 6s) (26 21%) 1275.0509\n",
            "5m 12s (- 17m 55s) (27 22%) 1275.3409\n",
            "5m 24s (- 17m 44s) (28 23%) 1271.3880\n",
            "5m 35s (- 17m 34s) (29 24%) 1263.5781\n",
            "5m 47s (- 17m 23s) (30 25%) 1261.5730\n",
            "5m 59s (- 17m 12s) (31 25%) 1253.4403\n",
            "6m 11s (- 17m 1s) (32 26%) 1251.4533\n",
            "6m 23s (- 16m 50s) (33 27%) 1253.4600\n",
            "6m 35s (- 16m 39s) (34 28%) 1248.7416\n",
            "6m 47s (- 16m 28s) (35 29%) 1245.7776\n",
            "6m 58s (- 16m 17s) (36 30%) 1239.7838\n",
            "7m 10s (- 16m 6s) (37 30%) 1229.7282\n",
            "7m 22s (- 15m 55s) (38 31%) 1233.7965\n",
            "7m 34s (- 15m 44s) (39 32%) 1226.9621\n",
            "7m 46s (- 15m 32s) (40 33%) 1226.4244\n",
            "7m 58s (- 15m 21s) (41 34%) 1224.6454\n",
            "8m 10s (- 15m 10s) (42 35%) 1227.5379\n",
            "8m 22s (- 14m 59s) (43 35%) 1217.7779\n",
            "8m 34s (- 14m 48s) (44 36%) 1216.7548\n",
            "8m 46s (- 14m 37s) (45 37%) 1207.2452\n",
            "8m 58s (- 14m 25s) (46 38%) 1208.5288\n",
            "9m 10s (- 14m 14s) (47 39%) 1208.1250\n",
            "9m 22s (- 14m 3s) (48 40%) 1207.1002\n",
            "9m 34s (- 13m 52s) (49 40%) 1199.8216\n",
            "9m 46s (- 13m 40s) (50 41%) 1200.1342\n",
            "9m 58s (- 13m 29s) (51 42%) 1197.7874\n",
            "10m 10s (- 13m 17s) (52 43%) 1195.2252\n",
            "10m 22s (- 13m 6s) (53 44%) 1190.2983\n",
            "10m 34s (- 12m 55s) (54 45%) 1187.2258\n",
            "10m 46s (- 12m 43s) (55 45%) 1187.6081\n",
            "10m 57s (- 12m 31s) (56 46%) 1181.5128\n",
            "11m 9s (- 12m 20s) (57 47%) 1183.7304\n",
            "11m 21s (- 12m 8s) (58 48%) 1181.4537\n",
            "11m 33s (- 11m 57s) (59 49%) 1175.5364\n",
            "11m 45s (- 11m 45s) (60 50%) 1173.0182\n",
            "11m 57s (- 11m 33s) (61 50%) 1175.3233\n",
            "12m 9s (- 11m 22s) (62 51%) 1167.7904\n",
            "12m 21s (- 11m 10s) (63 52%) 1165.2761\n",
            "12m 33s (- 10m 59s) (64 53%) 1163.7991\n",
            "12m 45s (- 10m 47s) (65 54%) 1162.8267\n",
            "12m 57s (- 10m 35s) (66 55%) 1156.5185\n",
            "13m 9s (- 10m 24s) (67 55%) 1159.5186\n",
            "13m 21s (- 10m 12s) (68 56%) 1155.7661\n",
            "13m 32s (- 10m 0s) (69 57%) 1156.6316\n",
            "13m 44s (- 9m 49s) (70 58%) 1155.4171\n",
            "13m 56s (- 9m 37s) (71 59%) 1154.6268\n",
            "14m 8s (- 9m 25s) (72 60%) 1153.7778\n",
            "14m 20s (- 9m 13s) (73 60%) 1149.1158\n",
            "14m 32s (- 9m 2s) (74 61%) 1144.3017\n",
            "14m 44s (- 8m 50s) (75 62%) 1145.0096\n",
            "14m 56s (- 8m 38s) (76 63%) 1144.4193\n",
            "15m 7s (- 8m 26s) (77 64%) 1143.9234\n",
            "15m 19s (- 8m 15s) (78 65%) 1144.3569\n",
            "15m 31s (- 8m 3s) (79 65%) 1140.6558\n",
            "15m 43s (- 7m 51s) (80 66%) 1138.0123\n",
            "15m 55s (- 7m 39s) (81 67%) 1133.0450\n",
            "16m 6s (- 7m 28s) (82 68%) 1128.4661\n",
            "16m 18s (- 7m 16s) (83 69%) 1127.6054\n",
            "16m 30s (- 7m 4s) (84 70%) 1126.9044\n",
            "16m 42s (- 6m 52s) (85 70%) 1125.5773\n",
            "16m 54s (- 6m 40s) (86 71%) 1124.6711\n",
            "17m 6s (- 6m 29s) (87 72%) 1123.5690\n",
            "17m 18s (- 6m 17s) (88 73%) 1123.7827\n",
            "17m 29s (- 6m 5s) (89 74%) 1115.9185\n",
            "17m 41s (- 5m 53s) (90 75%) 1116.6488\n",
            "17m 53s (- 5m 42s) (91 75%) 1117.7007\n",
            "18m 5s (- 5m 30s) (92 76%) 1116.3511\n",
            "18m 16s (- 5m 18s) (93 77%) 1110.8765\n",
            "18m 28s (- 5m 6s) (94 78%) 1111.5468\n",
            "18m 40s (- 4m 54s) (95 79%) 1108.2120\n",
            "18m 52s (- 4m 43s) (96 80%) 1106.9134\n",
            "19m 3s (- 4m 31s) (97 80%) 1104.1164\n",
            "19m 15s (- 4m 19s) (98 81%) 1105.2215\n",
            "19m 27s (- 4m 7s) (99 82%) 1107.0085\n",
            "19m 39s (- 3m 55s) (100 83%) 1100.4132\n",
            "19m 51s (- 3m 44s) (101 84%) 1098.5065\n",
            "20m 2s (- 3m 32s) (102 85%) 1097.7143\n",
            "20m 14s (- 3m 20s) (103 85%) 1099.3141\n",
            "20m 25s (- 3m 8s) (104 86%) 1100.5915\n",
            "20m 37s (- 2m 56s) (105 87%) 1096.8638\n",
            "20m 48s (- 2m 44s) (106 88%) 1095.7734\n",
            "21m 0s (- 2m 33s) (107 89%) 1090.7183\n",
            "21m 11s (- 2m 21s) (108 90%) 1093.0930\n",
            "21m 22s (- 2m 9s) (109 90%) 1095.1787\n",
            "21m 33s (- 1m 57s) (110 91%) 1091.2739\n",
            "21m 44s (- 1m 45s) (111 92%) 1089.3713\n",
            "21m 55s (- 1m 33s) (112 93%) 1091.5893\n",
            "22m 6s (- 1m 22s) (113 94%) 1089.6864\n",
            "22m 17s (- 1m 10s) (114 95%) 1086.4184\n",
            "22m 28s (- 0m 58s) (115 95%) 1086.5302\n",
            "22m 39s (- 0m 46s) (116 96%) 1080.2042\n",
            "22m 50s (- 0m 35s) (117 97%) 1081.9250\n",
            "23m 1s (- 0m 23s) (118 98%) 1078.4986\n",
            "23m 11s (- 0m 11s) (119 99%) 1078.0088\n",
            "23m 22s (- 0m 0s) (120 100%) 1081.8385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvqQe3Esnjc5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i,data in enumerate(dataloader) :\n",
        "    print(i)\n",
        "    print(shape(data[0]))\n",
        "    print((data[0]==data[1]).all())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEhzijeesMsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "input = torch.from_numpy(Xtrain[:batch_size]).float().to(device)\n",
        "#print(input.size())\n",
        "#input = input.view(1,162,-1)\n",
        "#print(input.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORGSbUejXTum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54fac36a-ec9f-4873-e7f3-094b9ca34212"
      },
      "source": [
        "shape(Xtrain)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12520, 162, 38)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWGzgZ1FXuI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def trainAttention(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCNG36ZHgs6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 162\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, feature_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.feature_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFT0G6xJNQGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "869514a7-0a46-43d4-8e91-f2f2beae4c7a"
      },
      "source": [
        "a = np.where(wordloop==\"carry\")[0]\n",
        "shape(timitceps[a[5]])\n",
        "#wordloop[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73, 38)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg9cMLmp71sm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca9f9a70-7b70-46c4-910f-6beae08ef418"
      },
      "source": [
        "wordloop[np.where(lenframes==lenframes.min())[0]]\n",
        "#wavfile.write('test.wav', 16000, new_timit_audio[15545])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['to', 'a'], dtype='<U6')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLm4OM2265Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e3a41aba-b487-45bb-8c85-b3924d6c11db"
      },
      "source": [
        "print(type(timitceps[0]))\n",
        "_,check = array(np.unique(wordloop,return_counts=True))\n",
        "print(sum(np.round(0.8*check.astype(float))))#/sum(check.astype(float)))\n",
        "(0.8*check.astype(float))\n",
        "sum(check.astype(float))\n",
        "a = array([2,3,4,5,6])\n",
        "cumsum(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "12520.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  5,  9, 14, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spw_p9eT2wis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a488d5a3-a028-42cd-c567-7b529e53bf02"
      },
      "source": [
        "print(lenframes.max())\n",
        "lenframes.min()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBi76r4XFwop",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "074c894c-7f70-45f6-dfc0-1010e151f9b0"
      },
      "source": [
        "print(words)\n",
        "print(freqs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['a' 'all' 'an' 'and' 'are' 'ask' 'be' 'carry' 'dark' \"don't\" 'for'\n",
            " 'greasy' 'had' 'he' 'his' 'in' 'is' 'like' 'me' 'of' 'oily' 'on' 'rag'\n",
            " 'she' 'suit' 'that' 'the' 'to' 'was' 'wash' 'water' 'we' 'with' 'year'\n",
            " 'you' 'your']\n",
            "[ 866  543  571  481  237  464  175  463  465  488  216  462  526  233\n",
            "  189  944  399  511  505  454  470  164  470  571  462  611 1602 1018\n",
            "  235  469  479  154  187  472  271  565]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eB__vH2HJWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b6308927-2e8f-4be8-be62-0ba7713b055f"
      },
      "source": [
        "files.download(\"test.wav\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_f4278679-5dba-491b-8ff6-df33c86fa0e3\", \"test.wav\", 83354)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C60Vk2qmqwp0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}